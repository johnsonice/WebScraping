{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "down all renren file links with both chinese and english\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from multiprocessing import Pool \n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/huang/Dev/projects/WebScraping/3_renren')\n",
    "#%%\n",
    "## get page number \n",
    "def get_page_num(result):\n",
    "    if result.status_code != 200:\n",
    "        return None\n",
    "    c=result.content\n",
    "    soup = BeautifulSoup(c,\"lxml\")\n",
    "    pages =  soup.find('div',{'class':'pages'}).findAll('a')\n",
    "    page_num = pages[-1].getText()\n",
    "    page_num = int(re.findall(r'\\d+',page_num)[0])\n",
    "    #print('Total Page number is: ' + str(page_num))\n",
    "    return page_num\n",
    "\n",
    "def extract_link(link):\n",
    "    result_sub = requests.get(link)\n",
    "    if result_sub.status_code != 200:\n",
    "        raise\n",
    "    c = result_sub.content\n",
    "    soup = BeautifulSoup(c,\"lxml\")\n",
    "    down_item = soup.find('div',{'class':'subtitle-links tc'}).find('h3').find('a')\n",
    "    #print(down_item)\n",
    "    d_link = down_item.get('href')\n",
    "    d_name = down_item.getText().strip()\n",
    "    \n",
    "    return (d_link,d_name)\n",
    "\n",
    "def check_sub_language(item):\n",
    "    language = item.find('dd').findAll('span')[0].getText()\n",
    "    if '中英' in language:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "## get download link from title link \n",
    "def get_down_link(item):\n",
    "    if check_sub_language(item):\n",
    "        link = domin + item.find('dt').find('a').get('href')\n",
    "        d_link,d_name = extract_link(link)\n",
    "        return (d_link,d_name)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_all_links(pn):\n",
    "    print(pn)\n",
    "    page_url = 'http://www.zmz2017.com/esubtitle?page=' + str(pn)\n",
    "    print('sending request')\n",
    "    result = requests.get(page_url)\n",
    "    print('request sent')\n",
    "    if result.status_code != 200:\n",
    "        raise ValueError('Did not get correct response from server!')\n",
    "    c = result.content\n",
    "    soup = BeautifulSoup(c,\"lxml\")\n",
    "    items = soup.find('div',{'class':'box subtitle-list'}).findAll('li')\n",
    "    res = [get_down_link(x) for x in items]\n",
    "    res = [x for x in res if x != False]\n",
    "    print('success')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "sending request\n",
      "sending request\n",
      "request sent\n",
      "request sent\n",
      "success\n",
      "success\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "## send the request\n",
    "url = \"http://www.zmz2017.com/esubtitle\"\n",
    "domin = \"http://www.zmz2017.com\"\n",
    "result = requests.get(url)\n",
    "page_num = get_page_num(result)\n",
    "if page_num == None :\n",
    "    raise ValueError('Bug in getting page num, request failed.')\n",
    "## get page number\n",
    "\n",
    "#%%\n",
    "#all_links = []\n",
    "#\n",
    "#for pn in range(1,3):\n",
    "#    res = get_all_links(pn)\n",
    "#    all_links.extend(res)\n",
    "#\n",
    "#print(len(all_links))\n",
    "# \n",
    "\n",
    "#%%\n",
    "\n",
    "    \n",
    "p = Pool(2)\n",
    "all_links_mp = p.map(get_all_links,range(1,3),chunksize =1)\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "print(len(all_links_mp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://tu.zmzjstu.com/ftp/2017/0205/e3e2f8bb8c1057cd5fbfda2dd83e1334.rar',\n",
       "  '跑调天后.Florence.Foster.Jenkins.2016.1080p.BluRay.REMUX.AVC.DTS-HD.MA.5.1.中英字幕-Fantopia.rar'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0205/735ae7788b686abd4908462f6d30e119.zip',\n",
       "  'santa.clarita.diet.s01e07.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0205/0ee00d8270cf1e41672c2c3de6cf3d4e.rar',\n",
       "  'Taboo.UK.S01E05.720p.HDTV.x264-ORGANiC.rar'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0205/aa19cb3f37a9ffa20c8c46c5d1c8d44f.zip',\n",
       "  'santa.clarita.diet.s01e06.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0205/2facbbda34be08f9513ed8a35d2a963a.zip',\n",
       "  'Sense8.S02E00.A.Christmas.Special.720p.WEBRip.X264-DEFLATE.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0205/90430805e679f96f3424d8cddd18e483.zip',\n",
       "  'santa.clarita.diet.s01e05.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/ce3d8fc937a969c335e65a11b7dfca6b.zip',\n",
       "  'Grimm.S06E04.720p.WEB-DL.DD5.1.H264-RARBG.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/c07f4bb76bf6a6f57785198bd726224b.zip',\n",
       "  'Hawaii.Five-0.2010.S07E15.720p.HDTV.X264-DIMENSION.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/1c38ed987a3b525f157db14ee67bef65.zip',\n",
       "  'Grimm.S06E05.720p.HDTV.x264-AVS.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/1a982bf272164abd84036d80ae8755e6.zip',\n",
       "  'Dr.Ken.S02E16.720p.HDTV.x264-AVS.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/705f0684e7afe4375f349afedeb8cdfe.zip',\n",
       "  'MacGyver.2016.S01E14.720p.HDTV.X264-DIMENSION.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/43d1d1318cad91b1750f4163e8d092db.zip',\n",
       "  'santa.clarita.diet.s01e03.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/18d4413ea8235a4bfaa3440de15fa93a.zip',\n",
       "  'santa.clarita.diet.s01e02.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/dca0d72cb776018cc54e39bcdbc72574.zip',\n",
       "  'The.Fosters.S04E11.720p.HDTV.x264-FLEET.小版本.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/505bb20807347d5440fd0973112407c0.zip',\n",
       "  'santa.clarita.diet.s01e01.720p.webrip.x264-skgtv.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/b2e45b2dc2a0bdb0a0ad41bb9e61cfea.rar',\n",
       "  'The.Vampire.Diaries.S08E11.720p.HDTV.X264-DIMENSION.rar'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/8bba1e1959724f61e4779c8eb522c22a.zip',\n",
       "  'Billy.Lynns.Long.Halftime.Walk.2016.720p.BluRay.x264-GECKOS.zip'),\n",
       " ('http://tu.zmzjstu.com/ftp/2017/0204/64011e0d188af9af733f5911f376adcc.rar',\n",
       "  'The.Expanse.S02E01-E02.720p.HDTV.x264-FLEET.rar')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links_mp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
