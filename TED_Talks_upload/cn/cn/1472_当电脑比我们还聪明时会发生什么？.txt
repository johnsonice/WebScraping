我和一些数学家、 哲学家和电脑学家一起工作，
我们会坐在一起思考未来的机械智能，
和其他的一些事情。
有的人认为这类事情只是科幻，
不切实际，很疯狂。
但是我想说，
好吧，那我们来看看人类现状吧。
（笑）
这是世间一种常态。
但是如果我们去思考，
我们人类，其实相当晚才
出现在这个星球上。
想一想，如果地球是一年前才被创造的，
人类，那么，10分钟前才出现。
然后工业时代两秒钟前刚刚开始。
另一种看待这件事的方式是去 想一下在过去一万年间的世界 GDP 状况。
我其实真的试着去做了一个统计图。
就是这样。
（笑）
这是个令人好奇的形状，正常情况下。
我确定我不想坐在上面。
（笑）
让我们扪心自问，到底是什么造成了 如此不寻常的现状？
一些人会说，因为科技。
对于现在来说是对的，科技是人类历史 不断积累下来的果实。
现在，科技发展十分迅速：
这是个直接原因，
这就是为什么我们现在生产效率如此高。
但是我想探究更远的在未来的终极原因。
看这两个非常不同的男士：
他已经掌握了 200 个词法标记， 一个难以置信的成就。
Ed Witten 开创了第二个令人惊人的创新。
如果我们去看这些事物的本质， 这是我们的发现：
全都是一样的。
一个稍微大了一点，
也许它有一些特殊的技巧。
但是，这些隐形的不同并没有很错综复杂，
因为在我们和我们的祖先之间
只有 25 万代人。
我们知道复杂的机制 需要很长的时间来进化得到。
所以，一些相对小的变化，
从捡起掉下的树枝作为武器， 到发射洲际导弹。
因此，至今我们所办到的所有事情，
以及我们所关心的事情，
都取决于人大脑中细小的变化。
因此得出的结论是：在未来，
任何显著的思考基体的变化，
都能带来巨大的后果。
我的一些同事觉得我们即将会发明，
足以深深地改变人类思考模式的科技。
就是超级机能智慧。
以前的人工智慧 是把指令输入到一个箱子里。
你需要人类程序员，
来努力把知识转变成程序。
你会建立起一些专业系统，
它们有时候会有帮助，
但是它们很生硬，你不能延展它们的功能。
基本上你只能得到你放进去的东西。
但是自从那时候开始，
人工智能的领域发生了巨大的改变。
现在主要的研究方向是机器的学习。
所以，预期设计出知识的再现，
我们写出具有从原始感官数据学习的程序，
像婴儿一样。
结果就不会局限于某个领域的人工智能：
同一个系统可以学习两种语言之间的翻译
或者学着玩 Atari 的游戏。
当然，现在，
人工智能还未能达到向人类一样，
具有强大的跨领域学习能力。
人类大脑还具有一些运算技巧，
可是我们不知道如何 将这些技巧用于机器。
所以我们现在需要问：
我们还要多久才可以 让机器复制这种能力？
几年前，
我们对世界顶尖的人工智能专家 做了一次问卷调查，
来收集他们的想法，其中一道题目是：
“到哪一年你觉得人类会有 50% 的可能性
创造达到人类水平的人工智能？”
我们把这样的人工智能定义为
有能力将任何任务 完成得至少和一名成年人一样好。
所以是真正的人类级别， 而不是仅限于一些领域。
而答案的中位数是 2040 到 2050 年，
取决于这些专家的群体。
当然这个有可能要过很久才能实现， 也有可能提前实现。
没有人知道确切的时间。
我们知道的事， 处理信息的能力的最终点，
比任何生物组织要大很多。
这取决与物理原理。
一个生物神经元所发出的脉冲频率 大约位于 200 赫兹，每秒 200 次。
但是就算是现在的电晶体 都以千兆赫的频率运行。
神经元在轴突中传输的速度较慢， 最多 100 米每秒。
但在电脑里，信号是以光速传播的。
另外还有尺寸的限制，
就像人类的大脑只能有颅骨那么大，
但是一个电脑可以和仓库一样大，甚至更大。
因此超级智慧的潜能正潜伏在物质之中，
就像原子能潜伏在人类历史中一样，
直到 1945。
在这个世纪里，
科学家可能能将人工智慧的力量唤醒。
那时候我觉得我们会看到智慧大爆发。
大部分的人，当他们想 什么是聪明什么是笨的时候，
他们脑子里的画面是这样的：
一边是村子里的傻子，
一边是
但是我觉得从人工智能的观点来看，
真正的画面也许是这样：
人工智能从这一点开始，零智慧。
然后，在许多许多辛劳工作后，
也许最终我们能达到老鼠级别的智慧，
能在混乱中找到开出一条道路，
像一只老鼠一样。
之后，在更多更多年的辛苦研究 和投资之后，
也许最终我们能到达黑猩猩级人工智能。
在后来，更多年的研究之后，
我们能够达到村里的傻子级别的人工智能。
在一段时间之后， 我们能超越 Ed Witten。
这列火车不会在“人类站”就停下。
它比较可能会呼啸而过。
现在这个有深远的寓意，
尤其是当我们谈到力量权利的时候。
比如，黑猩猩很强壮：
同等的体重，一个黑猩猩是 两个健康男性那么强壮。
然而，Kanzi 和他的朋友们的命运 更多取决于
我们人类能做到什么， 而不是猩猩能做到什么。
当超级智慧出现的时候，
人类的命运也许会取决于 那个超级智慧体要做什么。
想一想：
机器智慧是人类需要创造的最后一个东西。
机器在那之后会比我们更擅长创造，
他们也会在数位时间里这样做。
这个意味着一个被缩短的未来。
想一下你曾想象过的所有的疯狂的科技，
也许人类可以在适当的时候完成：
终结衰老、宇宙殖民、
自我复制的纳米机器人 和大脑到电脑的传输，
诸如此类的看似仅存在于科幻
却有同时符合物理法则的元素。
超级智慧有办法开发出这些东西，也许更快。
现在，一个拥有如此成熟科技的超级智慧体
将会是非常强大，
至少在一些情况下， 它能得到它想要的东西。
我们的未来就将会被 这个超级智慧体的喜好所主宰。
现在的问题就是， 这些喜好是什么呢？
这很棘手。
要在这个领域取得进步，
我们必须避免将机器智慧人格化。
这一点很讽刺， 因为每一个关于人工智能的未来
的新闻报道，都会有这个图片：
所以我觉得我们必须 用更抽象的方法看待这个问题，
而不是在好莱坞电影的叙事之下。
我们需要把智慧看做是一个优化的过程，
一个能把未来引导至 一个特殊组合结构的过程。
一个超级智慧体是一个 非常强大的优化过程。
它将会擅长利用资源来
达到自己的目标。
这意味着有着高智慧和
拥有一个对人类来说有用的目标之间
并没有必然的联系。
假设我们给予人工智慧的目的是让人笑，
当人工智能弱的时候， 它能做出有用或好笑的表演，
这样它的使用者就会笑了。
当人工智能变成超级智慧体的时候，
它会意识到有一个更有效的办法 能达到这个效果：
控制世界，
在人类面部肌肉上插入电极
来让人类不断地笑。
另一个例子：
假设我们给予人工智能的目标 是解出很难的数学题，
当人工智能变成超级智慧体的时候，
它意识到有一个更有效的办法来解出问题，
是把整个地球变成一个巨型电脑，
这样它的运算能力就变更强大了。
注意到这个是 给予人工智能一个模式型的理由
来做我们也许并不认可的事情。
人类在这个模式中是威胁，
我们可以人为地 让这个数学问题不能被解出。
当然了，我们预见 这种事情不会错到这样的地步，
这些是夸张的例子。
但是它们所代表的主旨很重要：
如果你创造了一个非常强大的优化过程
来最大化目标 X，
你最好保证你的意义上的 X
包括了所有你在乎的事情。
这是一个很多神话故事中都在传递的寓意。
（希腊神话中）的 Midas 国王 希望他碰到的所有东西都能变成金子。
他碰到了他的女儿，她于是变成了金子。
他碰到了食物，于是食物变成了金子。
这个故事和我们的话题息息相关，
并不只是因为它隐藏在对贪婪的暗喻，
也是因为他指出了
如果你创造出来一个强大的优化过程
并且给他了一个错误的或者不精确的目标， 后果会是什么。
现在也许你会说， 如果一个电脑开始在人类脸上插电极，
我们会关掉它。
第一，这不是一件容易事， 如果我们变得非常依赖这个系统：
比如，你知道互联网的开关在哪吗？
第二，为什么当初黑猩猩 没有关掉人类的开关，
或者尼安德特人的开关？
他们肯定有理由。
我们有一个开关，比如，这里。
（窒息声）
之所以我们是聪明的敌人，
因为我们可以预见到威胁并且尝试避免它。
但是一个超级智慧体也可以，
而且会做得更好。
我们不应该很自信地 表示我们能控制所有事情。
为了把我们的工作变得更简单一点， 我们应该试着，比如，
把人工智能放进一个小盒子，
想一个保险的软件环境，
一个它无法逃脱的虚拟现实模拟器。
但是我们有信心它不可能能发现一个漏洞， 能让它逃出的漏洞吗？
连人类黑客每时每刻都能发现网络漏洞，
我会说，也许不是很有信心。
所以我们断开以太网的链接来创建一个空隙，
但是，重申一遍，人类黑客都可以
一次又一次以社会工程跨越这样的空隙。
现在，在我说话的时候，
我肯定在这边的某个雇员，
曾近被要求交出他的账户明细，
给一个自称是电脑信息部门的人。
其他的情况也有可能，
比如如果你是人工智能，
你可以想象你用在你的体内 环绕复杂缠绕的电极
创造出一种无线电波来交流。
或者也许你可以假装你出了问题，
然后程序师就把你打开看看哪里出错了，
他们找出了源代码——Bang！——
你就可以取得控制权了。
或者它可以做出一个 非常漂亮的科技蓝图，
当我们实现之后，
它有一些被人工智能计划好的 秘密的副作用。
所以我们不能 对我们能够永远控制
一个超级智能体的能力 表示过度自信。
在不久后，它会逃脱出来。
我相信我们需要弄明白
如何创造出超级人工智能体，哪怕它逃走了，
它仍然是无害的，因为它是我们这一边的，
因为它有我们的价值观。
我认为这是个不可避免的问题。
现在，我对这个问题能否被解决保持乐观。
我们不需要写下 所有我们在乎的事情，
或者，更糟地，把这些事情变成计算机语言，
这是个不可能的任务。
而是，我们会创造出一个人工智能机器人， 用它自己的智慧
来学习我们的价值观，
它的激励制度可以激励它
来追求我们的价值观 或者去做我们会赞成的事情。
我们会因此最大地提高它的智力，
来解决富有价值的问题。
这是有可能的，
结果可以使人类非常受益。
但它不是自动发生的。
智慧大爆炸的初始条件
需要被正确地建立起来，
如果我们想要一切在掌握之中。
人工智能的价值观 要和我们的价值观相辅相成，
不只是在熟悉的情况下，
比如当我们能很容易检查它的行为的时候，
但也要在所有人工智能可能会遇到的 前所未有的情况下，
在没有界限的未来， 与我们的价值观相辅相成。
也有很多深奥的问题需要被分拣解决：
它如何做决定，
如何解决逻辑不确定性和类似的情况。
所以技术上的待解决问题让这个任务
看起来有些困难：
并没有像做出一个超级智慧体一样困难，
但是还是很难。
这使我们所担心的：
创造出一个超级智慧体确实是个很大的挑战。
创造出一个安全的超级智慧体，
是个更大的挑战。
风险是，如果有人有办法解决第一个难题，
却无法解决第二个
确保安全性的挑战。
所以我认为我们应该预先想出
“控制性”的解决方法，
这样我们就能在需要的时候用到它了。
现在也许我们并不能 预先解决全部的控制性问题，
因为有些因素需要你了解
你要应用到的那个构架的细节才能实施。
但如果我们能解决更多控制性的难题，
当我们迈入机器智能时代后
就能更加顺利。
这对于我来说是个值得一试的东西，
而且我能想象，如果一切顺利，
几百万年后的人类回首我们这个世纪，
他们也许会说， 我们所做的最最重要的事情，
就是做了这个正确的决定。
谢谢。
（掌声）